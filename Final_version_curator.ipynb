{
 "metadata": {
  "name": "Recent Earthquakes"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import urllib\nfrom pandas import read_json\nfrom pandas import DataFrame\nfrom datetime import datetime, timedelta\nfrom pytz import timezone\nimport pytz",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def Milli_to_DateTime(millisec): #converting millsec time data into recognizable time data \n    utc = pytz.utc # setting UTC \n    fmt = '%A, %B %d, %Y %H:%M:%S %Z' #setting output \n    all_converted=range(len(millisec)) #declare variable\n\n    for x in range(0,len(millisec)): #converting all millsec data into recognizable time data\n        utc_dt = utc.localize(datetime.utcfromtimestamp(millisec[x]/1000))\n        converted=utc_dt.strftime(fmt)\n        all_converted[x]=converted #boom! done!\n    return(all_converted) #booom returns ",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#DataCleaning function will scrap the data from http://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php and save it to your \n#local workspace. Input variable is going to verify which type of data you want to scrap. \n#valid input variables are \"past hour\", \"past day\", \"past 7days\", \"past 30days\"\n\ndef DataCleaning(whichdata): \n    \n    if not whichdata in [\"past hour\", \"past day\", \"past 7days\", \"past 30days\"]: #this if statement will verify if you put valid input or not\n        \n        print(\"Invalid input bro, gg. Valid inputs are \\\"past hour\\\", \\\"past day\\\", \\\"past 7days\\\", \\\"past 30days\\\"\") #error sign!\n    \n    else:\n    \n        if whichdata == \"past hour\": # scrap past hour data\n            url = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson'\n            data = read_json(urllib.urlopen(url),typ='series',convert_axes=True, dtype=True,convert_dates=True)\n         \n        elif whichdata == \"past day\": # scrap past day data\n            url = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson'\n            data = read_json(urllib.urlopen(url),typ='series',convert_axes=True, dtype=True,convert_dates=True)\n        \n        elif whichdata == \"past 7days\": # scrap past 7days data\n            url = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_week.geojson'\n            data = read_json(urllib.urlopen(url),typ='series',convert_axes=True, dtype=True,convert_dates=True)\n        \n        elif whichdata == \"past 30days\": # scrap past 30days data\n            url = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_month.geojson'\n            data = read_json(urllib.urlopen(url),typ='series',convert_axes=True, dtype=True,convert_dates=True)\n            \n        \n        numbers_of_newdata=data['metadata']['count'] #counting numbers of data \n       \n        Src=range(numbers_of_newdata) #declare variables\n        Eqid=range(numbers_of_newdata)\n        Time=range(numbers_of_newdata)\n        Lat=range(numbers_of_newdata)\n        Lon=range(numbers_of_newdata)\n        Depth=range(numbers_of_newdata)\n        Nst=range(numbers_of_newdata)\n        Region=range(numbers_of_newdata)\n        Magnitude=range(numbers_of_newdata)\n    \n    \n        for x in range(0,numbers_of_newdata) : #read data that we need and save it to corresponding variables in your local repository\n            Src[x]      = data['features'][x]['properties']['net']\n            Eqid[x]     = data['features'][x]['properties']['code']\n            Time[x]     = data['features'][x]['properties']['time']\n            Lat[x]      = data['features'][x]['geometry']['coordinates'][1]\n            Lon[x]      = data['features'][x]['geometry']['coordinates'][0]\n            Depth[x]    = data['features'][x]['geometry']['coordinates'][2]\n            Nst[x]      = data['features'][x]['properties']['nst']\n            Region[x]   = data['features'][x]['properties']['place']\n            Magnitude[x]=data['features'][x]['properties']['mag']\n        \n        converted_time= Milli_to_DateTime(Time)\n        rawdata= {'Src':Src, 'Eqid':Eqid, 'Datetime':converted_time , 'Lat':Lat , 'Lon':Lon ,'Magnitude':Magnitude,'Depth':Depth,'NST': Nst,'Region': Region }\n        #making data set by using variables that we obtained\n        converted_rawdata=DataFrame(rawdata,columns=['Src','Eqid','Datetime','Lat','Lon','Magnitude','Depth','NST','Region'])\n        #converting data set into data frame\n        without_NA_data=converted_rawdata.dropna(axis=0, how='any')\n        #droppppiiinnnggggg each row if it has at least one na value\n    \n        if without_NA_data.shape[0] == 0: #checking if the final dataframe is empty\n           \n            print(\"Data is not available since each row contains at least one NA. Empty DataFrame! rofl!\")\n\n        return(without_NA_data) #returning final dataframe",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "DataCleaning(\"Dayum\") #testing for invalid input",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "past_hour=DataCleaning(\"past hour\") #this will scrap past hour data and save it to your local worksapce\npast_hour",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "past_day=DataCleaning(\"past day\") #this will scrap past day data and save it to your local worksapce\npast_day",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "past_7days=DataCleaning(\"past 7days\") #this will scrap past 7days data and save it to your local worksapce\npast_7days[0:30] #printing  first 30 data",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "past_30days=DataCleaning(\"past 30days\") #this will scrap past 30days data and save it to your local worksapce\npast_30days[0:30] #printing first 30 ",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
